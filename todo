-PREPARE
  -crawl + fuzz
   -crawl
✓     -urls starter-pack <= user input   (what to scan)
✓     -crawl all urls recursively to get SRC/HREF DEPs
         #only GET => no params outside URL
      -lookup forms(not mentioned in article)
         #not actually described in article but might be important
         #the crawler still's shit
        -*main action*
        -save POST params & headers
	 #need to change parser to check childnodes?
✓     -save urls
✓     -add https and ?? protocols 
        #already supported by py.reqs
    -fuzz
      -go through all DEPs, sending multiple [fuzzed] requests
        -send regular get/post/head/put/delete/connect/options/trace/path
        -fuzz - MUTATION-based approach
          -http headers
            -COOKIES especially
          -query params (imitate <form>)
            -replace with same len strings
          -no changes in response same => static (no vars)
           it is parsing task, but can be solved prematurely
      -save them into database
        -init db
        -insert
  -parse raw requests to templates
    -select data from db
    -diff
    -replace VARS with PLACEHOLDERS
      -choose PLACEHOLDER symbol
    -save templates [a.k.a plain req with placeholders] into db
-EXECUTE
  -web-server
    -recieve request
      -keep TCP alive
      -save request? its HONEYPOT after all
    -find matching template
      -??? check article to get what i gotta do cos idk 2hard!!!
    -construct response
      -replace PLACEHOLDERS with VARS
    -respond
